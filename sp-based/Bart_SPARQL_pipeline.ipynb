{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e11a5701-cea3-4c38-8f84-27f84aba242d",
      "metadata": {
        "id": "e11a5701-cea3-4c38-8f84-27f84aba242d"
      },
      "source": [
        "## KQAPro BART Integrated Pipeline - Bart_SPARQL Setup\n",
        "\n",
        "This Jupyter Notebook is designed to set up the pipeline for the [KQAPro Baselines - Bart_SPARQL](https://github.com/shijx12/KQAPro_Baselines/tree/master/Bart_SPARQL) project. It provides steps for downloading the necessary datasets, organizing files, and preparing the environment to run the Bart_SPARQL code.\n",
        "\n",
        "Ensure that all dependencies are installed and the required tools are available in your system before proceeding."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2fd6584a",
      "metadata": {
        "id": "2fd6584a"
      },
      "source": [
        "> To Run it on **Colab**:\n",
        ">\n",
        "> 1. First, **upload** and open this jupyter **notebook** file  \n",
        ">\n",
        "> 2. Second, clone the related [github repository](https://github.com/Xchange7/NLP_KBQA) by executing the following command:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57ab1a4f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57ab1a4f",
        "outputId": "0091b257-3c59-42d0-d361-bf6ea26b55e1"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/Xchange7/NLP_KBQA.git"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd2b8b50",
      "metadata": {
        "id": "fd2b8b50"
      },
      "source": [
        "> 3. change the directory to `sp-based/`\n",
        ">\n",
        ">     Use `%cd` rather than `!cd` !!!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b553752",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2b553752",
        "outputId": "96c08224-faf4-45a0-c689-386e88828d60"
      },
      "outputs": [],
      "source": [
        "%cd NLP_KBQA/sp-based/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uwIdx7jpK12U",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uwIdx7jpK12U",
        "outputId": "70dc1f14-b763-432b-c58e-10439f9a4722"
      },
      "outputs": [],
      "source": [
        "!pwd"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8516955",
      "metadata": {
        "id": "d8516955"
      },
      "source": [
        "> 4. Now continue the following cells"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd8e79dd-fcc0-4968-ad9f-b1171f7fd98a",
      "metadata": {
        "id": "bd8e79dd-fcc0-4968-ad9f-b1171f7fd98a"
      },
      "source": [
        "### Download Datasets\n",
        "\n",
        "The following 4 jupyter cells will do the followings:\n",
        "\n",
        "- Download datasets `train.json`, `val.json` and `test.json` from [https://cloud.tsinghua.edu.cn/f/04ce81541e704a648b03/?dl=1](https://cloud.tsinghua.edu.cn/f/04ce81541e704a648b03/?dl=1)\n",
        "- Download datasets `kb.json` from [https://huggingface.co/datasets/drt/kqa_pro](https://huggingface.co/datasets/drt/kqa_pro)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b805c03-47e8-46ff-9077-57309ad35b21",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9b805c03-47e8-46ff-9077-57309ad35b21",
        "outputId": "40976c6f-dc44-4adf-c4b2-b40cc018c8d4"
      },
      "outputs": [],
      "source": [
        "# Simply run it\n",
        "\n",
        "!wget -O datasets.zip \"https://cloud.tsinghua.edu.cn/f/04ce81541e704a648b03/?dl=1\" \\\n",
        "&& unzip -o datasets.zip -d datasets \\\n",
        "&& mv datasets/KQAPro.IID/* datasets/ \\\n",
        "&& rm -r datasets/KQAPro.IID \\\n",
        "&& rm datasets.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "beec3dab-52e2-4145-9795-964fbb3bff4a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "beec3dab-52e2-4145-9795-964fbb3bff4a",
        "outputId": "3293e5bf-20bc-42c5-8b02-63365a6c90e8"
      },
      "outputs": [],
      "source": [
        "%ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e033209b-5085-4e91-94b8-944f05afe2ee",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e033209b-5085-4e91-94b8-944f05afe2ee",
        "outputId": "4d25b8e5-3b81-4073-9792-9ee77fb4d71b"
      },
      "outputs": [],
      "source": [
        "# Simply run it\n",
        "\n",
        "!wget -O datasets/kb.json \"https://huggingface.co/datasets/drt/kqa_pro/resolve/main/kb.json?download=true\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2eb65f55-aa46-41e8-ac9d-779fee897e57",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2eb65f55-aa46-41e8-ac9d-779fee897e57",
        "outputId": "d6bbb66e-5931-4141-9393-6072cece28b1"
      },
      "outputs": [],
      "source": [
        "%ls ./datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5696ed62",
      "metadata": {
        "id": "5696ed62"
      },
      "source": [
        "### Modify the datasets\n",
        "\n",
        "- Current structures of `train.json`, `val.json`:\n",
        "  ```json\n",
        "  {\n",
        "    \"question\": \"\",   // !!! input of the model\n",
        "    \"choices\": [],  // ignore this field\n",
        "    \"program\": [],  // ignore this field\n",
        "    \"sparql\": \"\",   // !!! output of the model\n",
        "    \"answer\": \"\"  // ignore this field\n",
        "  }\n",
        "  ```\n",
        "\n",
        "- Current structure of `test.json`:\n",
        "  ```json\n",
        "  {\n",
        "    \"question\": \"\",   // !!! input of the model\n",
        "    \"answer\": \"\"  // ignore this field\n",
        "  }  // PROBLEM: no `sparql` field\n",
        "  ```\n",
        "\n",
        "- Current `test.json` file has no `sparql` field, so we split the `val.json` into two parts, taking the last **5000** pieces of samples as new test set, and others as evaluation set.\n",
        "- At the same time, we also restructure the json format in all `train.json`, `val.json`, `test.json` files with proper **indentation**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "681a2d78",
      "metadata": {
        "id": "681a2d78"
      },
      "outputs": [],
      "source": [
        "!rm ./datasets/test.json"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c70d960d",
      "metadata": {
        "id": "c70d960d"
      },
      "source": [
        "Currently, all data are stored in **a single line** in each file, which is not human-readable. We will reformat the data to make it more readable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "534a5ba5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "534a5ba5",
        "outputId": "2e587fa5-9e4f-4b7e-a261-4e75597ca3fb"
      },
      "outputs": [],
      "source": [
        "!wc -l ./datasets/*.json  # calculate the number of lines in each file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b44f9c4d",
      "metadata": {
        "id": "b44f9c4d"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "\n",
        "with open('./datasets/val.json', 'r', encoding='utf-8') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "if len(data) != 11797:  # original number of samples in `val.json`\n",
        "    # the file has been restructured with proper indentation\n",
        "    raise Exception('The file `val.json` has been split into `val.json` and `test.json` already!\\nNo need to run this script again.')\n",
        "\n",
        "# fetch the last 5000 samples as test data\n",
        "test_data = data[-5000:]\n",
        "remaining_data = data[:-5000]\n",
        "\n",
        "with open('./datasets/test.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(test_data, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "with open('./datasets/val.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(remaining_data, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "# at the same time, restructure `train.json` with proper indentation\n",
        "with open('./datasets/train.json', 'r', encoding='utf-8') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "with open('./datasets/train.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(data, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "print('Successfully split `val.json` into `val.json` and `test.json`, and restructured all files with indentation.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec89a24a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ec89a24a",
        "outputId": "4e2edef8-44ef-4e35-c3a8-450021a75e2f"
      },
      "outputs": [],
      "source": [
        "!wc -l ./datasets/*.json  # calculate the number of lines in each file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f277af7c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f277af7c",
        "outputId": "18661404-8902-4b39-c984-454ad327d55b"
      },
      "outputs": [],
      "source": [
        "# OPTIONAL: convert `train.json`, `val.json`, and `test.json` to `jsonl` format\n",
        "\n",
        "!python json2jsonl.py --mode default"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1cba9303",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1cba9303",
        "outputId": "eefc1cf6-e44a-4a6f-a368-ef84a82aca89"
      },
      "outputs": [],
      "source": [
        "!wc -l ./datasets/*.jsonl  # calculate the number of lines in each file, which represents the number of samples"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d223c74d-d46d-4799-84f9-202a070f3816",
      "metadata": {
        "id": "d223c74d-d46d-4799-84f9-202a070f3816"
      },
      "source": [
        "### Configure rdflib package\n",
        "\n",
        "Follow the instructions in [https://github.com/shijx12/KQAPro_Baselines/tree/master/Bart_SPARQL#requirements](https://github.com/shijx12/KQAPro_Baselines/tree/master/Bart_SPARQL#requirements)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "f410d6a3-bb22-4171-8b60-60c78b2d9a34",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f410d6a3-bb22-4171-8b60-60c78b2d9a34",
        "outputId": "14342293-dd99-4bbc-b019-298fef7d1914"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: rdflib in /home/wsl/.local/lib/python3.8/site-packages (7.1.1)\n",
            "Requirement already satisfied: isodate<1.0.0,>=0.7.2 in /home/wsl/.local/lib/python3.8/site-packages (from rdflib) (0.7.2)\n",
            "Requirement already satisfied: pyparsing<4,>=2.1.0 in /home/wsl/.local/lib/python3.8/site-packages (from rdflib) (3.1.2)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install rdflib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3efa088f-a8c7-4a81-92eb-e25a4bdd5d3a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3efa088f-a8c7-4a81-92eb-e25a4bdd5d3a",
        "outputId": "befaa13f-031a-49a7-c756-4a62c6fd8ebe"
      },
      "outputs": [],
      "source": [
        "import rdflib\n",
        "import os\n",
        "\n",
        "\"\"\" Follow the instructions of the output below: \"\"\"\n",
        "\n",
        "base_dir = os.path.dirname(rdflib.__file__)\n",
        "print(f\"base_dir: {base_dir}\")\n",
        "\n",
        "file1 = os.path.join(base_dir, \"plugins/sparql/parser.py\")\n",
        "file2 = os.path.join(base_dir, \"plugins/serializers/turtle.py\")\n",
        "\n",
        "\"\"\"What you need TODO:\"\"\"\n",
        "print(\"\\nThere are 2 files to change in total.\")\n",
        "print(f\"File1: {file1}\")\n",
        "print(f\"File2: {file2}\")\n",
        "\n",
        "print(f\"\"\"\n",
        "First, edit file1, replace the line with codes:\n",
        "`if i + 1 < l and (not isinstance(terms[i + 1], str) or terms[i + 1] not in \".,;\"):`\n",
        "which is just below the line `# is this bnode the subject of more triplets?`\n",
        "\"\"\", end=\"\")\n",
        "\n",
        "print(f\"\"\"\n",
        "Second, edit file2, replace `use_plain=True` with `use_plain=False`\n",
        "\"\"\")\n",
        "\n",
        "print(\"For more detailed information, check https://github.com/shijx12/KQAPro_Baselines/tree/master/SPARQL#requirements\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wHZc03euLHNH",
      "metadata": {
        "id": "wHZc03euLHNH"
      },
      "source": [
        "Now edit file1 and file2.\n",
        "\n",
        "> For Colab users, the file paths could be:  \n",
        ">\n",
        "> File1: /usr/local/lib/python3.10/dist-packages/rdflib/plugins/sparql/parser.py  \n",
        ">\n",
        "> File2: /usr/local/lib/python3.10/dist-packages/rdflib/plugins/serializers/turtle.py  \n",
        ">\n",
        "> Simply click the file links to edit"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ad52a04-9360-4203-847f-b5221e1eabfc",
      "metadata": {
        "id": "5ad52a04-9360-4203-847f-b5221e1eabfc"
      },
      "source": [
        "### Configure SPARQLWrapper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab050a27-3804-4816-b2f6-c731d3e64cce",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ab050a27-3804-4816-b2f6-c731d3e64cce",
        "outputId": "82592945-6853-48db-fba5-10f40473d967"
      },
      "outputs": [],
      "source": [
        "%pip install SPARQLWrapper==1.8.4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d518e511-c41b-4042-80a9-44967f5e515b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d518e511-c41b-4042-80a9-44967f5e515b",
        "outputId": "1856ed68-da24-4322-8f6c-7036f32dba0c"
      },
      "outputs": [],
      "source": [
        "%pip show keepalive  # Make sure `keepalive` NOT installed"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2501d07c-85b3-4d27-b37a-1801ef9606f9",
      "metadata": {
        "id": "2501d07c-85b3-4d27-b37a-1801ef9606f9"
      },
      "source": [
        "### Virtuoso Configuration\n",
        "\n",
        "> *Needed for validation and evaluation (Executing SPARQL query to local Virtuoso database)*\n",
        "\n",
        "- The virtuoso backend will start up a web service, we can import our kb into it and then execute SPARQL queries by network requests.\n",
        "- **Purpose of Virtuoso**: The primary purpose of this configuration is to install and set up the Virtuoso backend service on an Ubuntu system, enabling the import of a **knowledge base (KB)** and facilitating access and operations on the data through the **SPARQL query interface**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e7d923a-41e8-40ac-b9bf-282b92aee9fd",
      "metadata": {
        "id": "2e7d923a-41e8-40ac-b9bf-282b92aee9fd"
      },
      "source": [
        "Follow the steps in [https://github.com/shijx12/KQAPro_Baselines/tree/master/Bart_SPARQL#how-to-install-virtuoso-backend](https://github.com/shijx12/KQAPro_Baselines/tree/master/Bart_SPARQL#how-to-install-virtuoso-backend)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8eaff19f",
      "metadata": {
        "id": "8eaff19f"
      },
      "source": [
        "### Loguru Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb20ca86",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cb20ca86",
        "outputId": "7e1149ef-d42a-4a46-d3e8-87805d01b63a"
      },
      "outputs": [],
      "source": [
        "%pip install loguru"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca6b7ba8",
      "metadata": {},
      "source": [
        "### Download the pre-trained BART model\n",
        "\n",
        "1. The pretrained **Bart-base checkpoint** without finetuning can be downloaded here [bart-base](https://cloud.tsinghua.edu.cn/f/3b59ec6c43034cfc8841/?dl=1)\n",
        "2. The checkpoint for **finetuned Bart_SPARQL** can be downloaded here [finetuned](https://cloud.tsinghua.edu.cn/f/1b9746dcd96b4fca870d/?dl=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3e79641",
      "metadata": {},
      "outputs": [],
      "source": [
        "!wget -O bart_base_model.zip \"https://cloud.tsinghua.edu.cn/f/3b59ec6c43034cfc8841/?dl=1\" \\\n",
        "&& unzip -o bart_base_model.zip -d bart_base_model \\\n",
        "&& rm bart_base_model.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b96db1cf-2994-4b60-a454-5ae07852351a",
      "metadata": {
        "id": "b96db1cf-2994-4b60-a454-5ae07852351a"
      },
      "source": [
        "### Preprocess the training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ae20098-be27-484c-b327-6d189b5a199c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ae20098-be27-484c-b327-6d189b5a199c",
        "outputId": "d138fc3d-a605-4711-c7e9-644e916f074d"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "7a62e93b-707c-44fc-a173-17209ed9eec9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7a62e93b-707c-44fc-a173-17209ed9eec9",
        "outputId": "193e2d33-b9d9-4846-8bdf-8e0caf5cf89a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Build kb vocabulary\n",
            "Load questions\n",
            "Dump vocab to bart_processed_data/vocab.json\n",
            "answer_token_to_idx:79329\n",
            "vocab.json: 100%|████████████████████████████| 899k/899k [00:00<00:00, 11.9MB/s]\n",
            "merges.txt: 100%|████████████████████████████| 456k/456k [00:00<00:00, 2.49MB/s]\n",
            "tokenizer.json: 100%|██████████████████████| 1.36M/1.36M [00:00<00:00, 3.84MB/s]\n",
            "loading file vocab.json from cache at /home/wsl/.cache/huggingface/hub/models--facebook--bart-base/snapshots/aadd2ab0ae0c8268c7c9693540e9904811f36177/vocab.json\n",
            "loading file merges.txt from cache at /home/wsl/.cache/huggingface/hub/models--facebook--bart-base/snapshots/aadd2ab0ae0c8268c7c9693540e9904811f36177/merges.txt\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at None\n",
            "loading file tokenizer.json from cache at /home/wsl/.cache/huggingface/hub/models--facebook--bart-base/snapshots/aadd2ab0ae0c8268c7c9693540e9904811f36177/tokenizer.json\n",
            "config.json: 100%|█████████████████████████| 1.72k/1.72k [00:00<00:00, 1.06MB/s]\n",
            "loading configuration file config.json from cache at /home/wsl/.cache/huggingface/hub/models--facebook--bart-base/snapshots/aadd2ab0ae0c8268c7c9693540e9904811f36177/config.json\n",
            "Model config BartConfig {\n",
            "  \"_name_or_path\": \"facebook/bart-base\",\n",
            "  \"activation_dropout\": 0.1,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"add_bias_logits\": false,\n",
            "  \"add_final_layer_norm\": false,\n",
            "  \"architectures\": [\n",
            "    \"BartModel\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classif_dropout\": 0.1,\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 6,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 6,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"model_type\": \"bart\",\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"normalize_before\": false,\n",
            "  \"normalize_embedding\": true,\n",
            "  \"num_beams\": 4,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"scale_embedding\": false,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 128,\n",
            "      \"min_length\": 12,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_cnn\": {\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 142,\n",
            "      \"min_length\": 56,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_xsum\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 62,\n",
            "      \"min_length\": 11,\n",
            "      \"num_beams\": 6\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.46.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "Encode train set\n",
            "100%|████████████████████████████████| 94376/94376 [00:00<00:00, 1490019.78it/s]\n",
            "dict_keys(['input_ids', 'attention_mask'])\n",
            "[0, 32251, 1139, 34, 10, 3842, 2688, 9, 204, 45121, 406, 34916, 3416, 1360, 8, 34, 41, 8192, 7961, 5135, 9, 6178, 398, 35534, 116, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "<s>Which town has a TOID of 4000000074573917 and has an OS grid reference of SP8778?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "<s>SELECT DISTINCT ?qpv WHERE { ?e <pred:instance_of> ?c . ?c <pred:name> \"visual artwork\" . ?e <official_website> ?pv_1 . ?pv_1 <pred:value> \"http://www.thesiege.com/\" . ?e <publication_date> ?pv . ?pv <pred:date> \"1999-01-21\"^^xsd:date . [ <pred:fact_h> ?e ; <pred:fact_r> <publication_date> ; <pred:fact_t> ?pv ] <place_of_publication> ?qpv .  }</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "304\n",
            "100%|█████████████████████████████████| 94376/94376 [00:00<00:00, 225775.41it/s]\n",
            "/home/wsl/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "shape of input_ids of questions, attention_mask of questions, input_ids of sparqls, choices and answers:\n",
            "(94376, 304)\n",
            "(94376, 304)\n",
            "(94376, 304)\n",
            "(94376, 10)\n",
            "(94376,)\n",
            "Encode val set\n",
            "100%|██████████████████████████████████| 6797/6797 [00:00<00:00, 1560324.25it/s]\n",
            "dict_keys(['input_ids', 'attention_mask'])\n",
            "[0, 12375, 21, 5, 4588, 1924, 77, 3801, 4, 3635, 8538, 300, 5, 3536, 3683, 13, 2700, 8123, 6, 27738, 196, 14828, 5785, 116, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "<s>Who was the prize winner when Mrs. Miniver got the Academy Award for Best Writing, Adapted Screenplay?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "<s>SELECT DISTINCT ?qpv WHERE { ?e <pred:instance_of> ?c . ?c <pred:name> \"television station\" . ?e_1 <original_network> ?e . ?e_1 <pred:name> \"Aladdin\" . ?e_1 <genre> ?e_2 . ?e_2 <pred:name> \"television comedy\" .  ?e <official_website> ?pv . ?pv <pred:value> \"http://www.disneychannel.com/\" . [ <pred:fact_h> ?e ; <pred:fact_r> <official_website> ; <pred:fact_t> ?pv ] <language_of_work_or_name> ?qpv .  }</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "288\n",
            "100%|███████████████████████████████████| 6797/6797 [00:00<00:00, 176400.93it/s]\n",
            "shape of input_ids of questions, attention_mask of questions, input_ids of sparqls, choices and answers:\n",
            "(6797, 288)\n",
            "(6797, 288)\n",
            "(6797, 288)\n",
            "(6797, 10)\n",
            "(6797,)\n",
            "Encode test set\n",
            "100%|██████████████████████████████████| 5000/5000 [00:00<00:00, 1339092.01it/s]\n",
            "dict_keys(['input_ids', 'attention_mask'])\n",
            "[0, 6209, 10433, 16154, 5, 599, 40952, 9, 5, 621, 19, 3703, 17640, 47489, 16273, 2663, 501, 1898, 9465, 3570, 116, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "<s>Is Socceroos the Twitter username of the person with ISNI 0000 0001 1445 0107?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "<s>How is Luzerne County related to Wilkes-Barre?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "108\n",
            "100%|███████████████████████████████████| 5000/5000 [00:00<00:00, 189609.06it/s]\n",
            "shape of input_ids of questions, attention_mask of questions, input_ids of sparqls, choices and answers:\n",
            "(5000, 108)\n",
            "(5000, 108)\n",
            "(0,)\n",
            "(5000, 10)\n",
            "(0,)\n"
          ]
        }
      ],
      "source": [
        "!python3 -m Bart_SPARQL.preprocess --input_dir datasets --output_dir bart_processed_data \\\n",
        "    --model_name_or_path facebook/bart-base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "51d143a9-8f7b-49b2-b68c-910775510b14",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51d143a9-8f7b-49b2-b68c-910775510b14",
        "outputId": "7ec8d1fb-e7d4-4208-e53b-bd5e93a2fd85"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "bart_processed_data:\n",
            "\u001b[0m\u001b[01;32mtest.pt\u001b[0m*  \u001b[01;32mtrain.pt\u001b[0m*  \u001b[01;32mval.pt\u001b[0m*  \u001b[01;32mvocab.json\u001b[0m*\n",
            "\n",
            "datasets:\n",
            "\u001b[01;32mkb.json\u001b[0m*  \u001b[01;32mREADME.md\u001b[0m*  \u001b[01;32mtest.jsonl\u001b[0m*  \u001b[01;32mtrain.jsonl\u001b[0m*  \u001b[01;32mval.jsonl\u001b[0m*\n",
            "\u001b[01;32mkb.ttl\u001b[0m*   \u001b[01;32mtest.json\u001b[0m*  \u001b[01;32mtrain.json\u001b[0m*  \u001b[01;32mval.json\u001b[0m*\n"
          ]
        }
      ],
      "source": [
        "%ls datasets bart_processed_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "e83026cf-86d4-4e82-9e2a-85c1cf8fde0a",
      "metadata": {
        "id": "e83026cf-86d4-4e82-9e2a-85c1cf8fde0a"
      },
      "outputs": [],
      "source": [
        "!cp ./datasets/kb.json bart_processed_data/"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1511c61b-a00a-4264-afce-a9b947171e9a",
      "metadata": {
        "id": "1511c61b-a00a-4264-afce-a9b947171e9a"
      },
      "source": [
        "### Train\n",
        "\n",
        "**BUG here!!!**:  \n",
        "\n",
        "There is a bug when the command below is executed with GPU, which can be fixed by editing the file:   \n",
        "`....../dist-packages/torch/nn/utils/rnn.py`  \n",
        "In Colab, the file path is:  \n",
        "`/usr/local/lib/python3.10/dist-packages/torch/nn/utils/rnn.py: line 338`\n",
        "\n",
        "Add `lengths = lengths.cpu()` before the line `data, batch_sizes = _VF._pack_padded_sequence(input, lengths, batch_first)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c109e198-cf8d-434c-9f43-a618f3279326",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c109e198-cf8d-434c-9f43-a618f3279326",
        "outputId": "72e0f35b-d7bb-4e07-aec8-6db45b897308"
      },
      "outputs": [],
      "source": [
        "# without GPU\n",
        "# !python3 -m SPARQL.train --input_dir processed_data/ --save_dir checkpoints/ --virtuoso_enabled False --num_epoch 1\n",
        "# !python3 -m Bart_SPARQL.train --input_dir bart_processed_data --output_dir bart_checkpoints --model_name_or_path facebook/bart-base --save_dir bart_checkpoints --num_epoch 1\n",
        "\n",
        "# with GPU\n",
        "# Run on Colab: --virtuoso_enabled False, there is no Virtuoso on Colab, no validating when training\n",
        "!CUDA_VISIBLE_DEVICES=0 python -m Bart_SPARQL.train --input_dir bart_processed_data --output_dir bart_checkpoints --model_name_or_path facebook/bart-base --save_dir bart_checkpoints"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c8df0ce",
      "metadata": {
        "id": "0c8df0ce"
      },
      "source": [
        "### Test\n",
        "\n",
        "On Colab: It's unable to run the test command without configuration of Virtuoso Service"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c922a11f",
      "metadata": {},
      "source": [
        "**'single' mode**: processes one model file only. `--save_dir` must be a path to a single .pt file\n",
        "\n",
        "**'multiple' mode**: processes all model files in a directory. `--save_dir` must be a path to a directory containing multiple .pt files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4161ab87-26ba-4a71-863d-bfab745daea7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4161ab87-26ba-4a71-863d-bfab745daea7",
        "outputId": "380f210b-6c4c-42df-c2c4-ca5277d36110"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "'single' mode: replace 'checkpoints/model_epoch0.pt' with your own model path\n",
        "\"\"\"\n",
        "\n",
        "# without GPU\n",
        "!python -m SPARQL.test --input_dir processed_data --save_dir checkpoints/model_epoch0.pt --mode single --results_dir test_results\n",
        "\n",
        "# with GPU\n",
        "!CUDA_VISIBLE_DEVICES=0 python -m SPARQL.test --input_dir processed_data --save_dir checkpoints/model_epoch0.pt --mode single --results_dir test_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UQIsX1vfgLQL",
      "metadata": {
        "id": "UQIsX1vfgLQL"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "'multiple' mode: replace 'checkpoints' with your own directory path of all .pt files\n",
        "\"\"\"\n",
        "\n",
        "# without GPU\n",
        "!python3 -m SPARQL.test --input_dir processed_data --save_dir checkpoints_from_colab --mode multiple --results_dir test_results\n",
        "\n",
        "# with GPU\n",
        "!CUDA_VISIBLE_DEVICES=0 python -m SPARQL.test --input_dir processed_data --save_dir checkpoints --mode multiple --results_dir test_results"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
